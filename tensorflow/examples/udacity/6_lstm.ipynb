{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set with size 99999000 :  ons anarchists advocate social relations based upon voluntary as\n",
      "valid set 1000 :   anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(\"train set with size \" + str(train_size) +\" : \", train_text[:64])\n",
    "print(\"valid set\" , valid_size,\": \", valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining of text:  ons anarchists advocate social relations based upon voluntary as\n",
      "Initial cursor:  [0, 1562484, 3124968, 4687452, 6249936, 7812420, 9374904, 10937388, 12499872, 14062356, 15624840, 17187324, 18749808, 20312292, 21874776, 23437260, 24999744, 26562228, 28124712, 29687196, 31249680, 32812164, 34374648, 35937132, 37499616, 39062100, 40624584, 42187068, 43749552, 45312036, 46874520, 48437004, 49999488, 51561972, 53124456, 54686940, 56249424, 57811908, 59374392, 60936876, 62499360, 64061844, 65624328, 67186812, 68749296, 70311780, 71874264, 73436748, 74999232, 76561716, 78124200, 79686684, 81249168, 82811652, 84374136, 85936620, 87499104, 89061588, 90624072, 92186556, 93749040, 95311524, 96874008, 98436492]\n",
      "batch  0 : text[ 0 ]=  o\n",
      "batch  1 : text[ 1562484 ]=  w\n",
      "batch  2 : text[ 3124968 ]=  l\n",
      "batch  3 : text[ 4687452 ]=   \n",
      "batch  4 : text[ 6249936 ]=  m\n",
      "batch  5 : text[ 7812420 ]=  h\n",
      "batch  6 : text[ 9374904 ]=  y\n",
      "batch  7 : text[ 10937388 ]=  a\n",
      "batch  8 : text[ 12499872 ]=  t\n",
      "batch  9 : text[ 14062356 ]=  m\n",
      "batch  10 : text[ 15624840 ]=  n\n",
      "batch  11 : text[ 17187324 ]=  h\n",
      "batch  12 : text[ 18749808 ]=  e\n",
      "batch  13 : text[ 20312292 ]=  e\n",
      "batch  14 : text[ 21874776 ]=  o\n",
      "batch  15 : text[ 23437260 ]=  y\n",
      "batch  16 : text[ 24999744 ]=  o\n",
      "batch  17 : text[ 26562228 ]=  a\n",
      "batch  18 : text[ 28124712 ]=   \n",
      "batch  19 : text[ 29687196 ]=  a\n",
      "batch  20 : text[ 31249680 ]=  i\n",
      "batch  21 : text[ 32812164 ]=   \n",
      "batch  22 : text[ 34374648 ]=  t\n",
      "batch  23 : text[ 35937132 ]=  d\n",
      "batch  24 : text[ 37499616 ]=  f\n",
      "batch  25 : text[ 39062100 ]=  a\n",
      "batch  26 : text[ 40624584 ]=  e\n",
      "batch  27 : text[ 42187068 ]=  e\n",
      "batch  28 : text[ 43749552 ]=  a\n",
      "batch  29 : text[ 45312036 ]=  r\n",
      "batch  30 : text[ 46874520 ]=  i\n",
      "batch  31 : text[ 48437004 ]=  o\n",
      "batch  32 : text[ 49999488 ]=  a\n",
      "batch  33 : text[ 51561972 ]=  g\n",
      "batch  34 : text[ 53124456 ]=  i\n",
      "batch  35 : text[ 54686940 ]=  r\n",
      "batch  36 : text[ 56249424 ]=  c\n",
      "batch  37 : text[ 57811908 ]=  a\n",
      "batch  38 : text[ 59374392 ]=   \n",
      "batch  39 : text[ 60936876 ]=  m\n",
      "batch  40 : text[ 62499360 ]=  t\n",
      "batch  41 : text[ 64061844 ]=  u\n",
      "batch  42 : text[ 65624328 ]=  e\n",
      "batch  43 : text[ 67186812 ]=  o\n",
      "batch  44 : text[ 68749296 ]=  o\n",
      "batch  45 : text[ 70311780 ]=  s\n",
      "batch  46 : text[ 71874264 ]=  k\n",
      "batch  47 : text[ 73436748 ]=  e\n",
      "batch  48 : text[ 74999232 ]=  w\n",
      "batch  49 : text[ 76561716 ]=  e\n",
      "batch  50 : text[ 78124200 ]=  t\n",
      "batch  51 : text[ 79686684 ]=  e\n",
      "batch  52 : text[ 81249168 ]=   \n",
      "batch  53 : text[ 82811652 ]=  i\n",
      "batch  54 : text[ 84374136 ]=  t\n",
      "batch  55 : text[ 85936620 ]=  d\n",
      "batch  56 : text[ 87499104 ]=  t\n",
      "batch  57 : text[ 89061588 ]=  e\n",
      "batch  58 : text[ 90624072 ]=  f\n",
      "batch  59 : text[ 92186556 ]=  d\n",
      "batch  60 : text[ 93749040 ]=  t\n",
      "batch  61 : text[ 95311524 ]=  a\n",
      "batch  62 : text[ 96874008 ]=  a\n",
      "batch  63 : text[ 98436492 ]=  s\n",
      "Begining of text:   anarchism originated as a term of abuse first used against earl\n",
      "Initial cursor:  [0]\n",
      "batch  0 : text[ 0 ]=   \n",
      "\n",
      "Example Train batch1: ['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "\n",
      "Example Train batch2: ['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "\n",
      "Example Valid batch1: [' an']\n",
      "\n",
      "Example Valid batch2: ['nar']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    print(\"Begining of text: \" , text[:64])\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size # // : floor division -> segment = number of batches\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] # indices of first element of a batch\n",
    "    print(\"Initial cursor: \" , self._cursor)\n",
    "    self._last_batch = self._next_batch(verbose = True)\n",
    "  \n",
    "  def _next_batch(self, verbose = False):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    # Each position in a batch follows a letter at the same position in the previous batch...\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      if verbose:\n",
    "        print(\"batch \", b, \": text[\", self._cursor[b],\"]= \" , self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # last_batch is the last character for every batch.\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(\"\\nExample Train batch1:\" , batches2string(train_batches.next()))\n",
    "print(\"\\nExample Train batch2:\", batches2string(train_batches.next()))\n",
    "print(\"\\nExample Valid batch1:\", batches2string(valid_batches.next()))\n",
    "print(\"\\nExample Valid batch2:\" , batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1): # num_nrollings : length of a single sequence ( there is batch_number of sequences in a batch.)\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298181 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "wctwdee  iznbek ro ja fptbom oe eemerhmrbuioaeti nw  dbgrfuk ke wzae ctg ri rago\n",
      "h  ypnel f iozsnkyka y  q  lo akli ebkzwqam on  tawoyn mmtncp s w rttd mafdwtut \n",
      "kul l fzdph e rfac yeekvbgsu r  etbt famhejjemlxts   nice tpkocpe h ea g ldnibpg\n",
      "ctuc etah dha k zrgtbisllyeziqxbre nniy iox efota  ahuepqqp  ilipcyeqpq gnutsat \n",
      "wezmge daih qpgmeebcsyftapzqigb  vowmadst  daltnipj t   eagsm pqle qxeieajauglee\n",
      "================================================================================\n",
      "Validation set perplexity: 20.83\n",
      "Average loss at step 100: 2.581827 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 13.85\n",
      "Average loss at step 200: 2.240139 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.52\n",
      "Validation set perplexity: 14.90\n",
      "Average loss at step 300: 2.098517 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 15.53\n",
      "Average loss at step 400: 1.998437 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 15.83\n",
      "Average loss at step 500: 1.936905 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 17.10\n",
      "Average loss at step 600: 1.911160 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 18.44\n",
      "Average loss at step 700: 1.857761 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 18.95\n",
      "Average loss at step 800: 1.817788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 18.63\n",
      "Average loss at step 900: 1.827341 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 18.04\n",
      "Average loss at step 1000: 1.824160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "kerdy as in hill emint kivliatide for the scrict was over walls as nith of scond\n",
      "g regrace over der histemman dralks beconk have fer five eight zero misuple of d\n",
      "y diof of narcongy sover e of the note reto sueduceer lebshill at feew the face \n",
      "uation invain and comprelly creatile listerive to the with travuati grember imen\n",
      "det orenk of hix form for doves new to and amplein mathur basely preteards in an\n",
      "================================================================================\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 1100: 1.773914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 20.84\n",
      "Average loss at step 1200: 1.752287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 18.98\n",
      "Average loss at step 1300: 1.734937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 1400: 1.746164 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 1500: 1.740220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 21.14\n",
      "Average loss at step 1600: 1.748021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 1700: 1.710211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 1800: 1.679139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 22.46\n",
      "Average loss at step 1900: 1.648955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 22.68\n",
      "Average loss at step 2000: 1.694735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "rish devilion resouply setten the sport copenrious are are three one nine nine o\n",
      "n modvinim eight as thfons as fourded sprission dlecages and jest bates universe\n",
      "crating of shorce maceer sopeth prodided to the lade wiald dinised souttersly tw\n",
      "m factions in fablinism as windhabicial of the vines juald lipthe aseing tolly t\n",
      "hes ausi ita woy ardoikwhity repilla it bal sears prograscus invourcols the ster\n",
      "================================================================================\n",
      "Validation set perplexity: 22.31\n",
      "Average loss at step 2100: 1.685567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 24.13\n",
      "Average loss at step 2200: 1.681770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 23.01\n",
      "Average loss at step 2300: 1.645870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 23.94\n",
      "Average loss at step 2400: 1.659497 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 23.52\n",
      "Average loss at step 2500: 1.682356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 22.98\n",
      "Average loss at step 2600: 1.657669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 23.55\n",
      "Average loss at step 2700: 1.659798 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 23.77\n",
      "Average loss at step 2800: 1.652336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 23.23\n",
      "Average loss at step 2900: 1.658662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 3000: 1.654409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "use is one in one nine six seven zero govers f a libea as a murine thin the afti\n",
      "pond arpur is systemption of p airspi an aru n domated by latters left with thea\n",
      "ing a roch fabilical proper the dure ficks mayito considered portant osts with p\n",
      "s artione a cayor and even travere tendenfe apistych it s are coust of the suppo\n",
      "preaphoralary theillowy is grourge in one nine vid the war destracies of aslance\n",
      "================================================================================\n",
      "Validation set perplexity: 23.82\n",
      "Average loss at step 3100: 1.631210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 22.98\n",
      "Average loss at step 3200: 1.650229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 23.90\n",
      "Average loss at step 3300: 1.641216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 25.22\n",
      "Average loss at step 3400: 1.673308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 24.90\n",
      "Average loss at step 3500: 1.661196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 24.74\n",
      "Average loss at step 3600: 1.671284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 23.98\n",
      "Average loss at step 3700: 1.651647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 24.40\n",
      "Average loss at step 3800: 1.647823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 23.75\n",
      "Average loss at step 3900: 1.639075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 23.85\n",
      "Average loss at step 4000: 1.656744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "ject b s accorfell external prodect of adigai repausing up leversion batinatine \n",
      "ques of phoger fremifs and first classique wears ocealers on one nine one nine t\n",
      "n marreith is cyrued of housbued from the regented and similal attace have this \n",
      "reed freeddoning five six zero zero zero zero zero zero five nine one nine five \n",
      "for traderred peroppentize very bell boolded in orias rederus notes cultatlehes \n",
      "================================================================================\n",
      "Validation set perplexity: 24.48\n",
      "Average loss at step 4100: 1.636413 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 23.72\n",
      "Average loss at step 4200: 1.638864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 24.03\n",
      "Average loss at step 4300: 1.613638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 24.72\n",
      "Average loss at step 4400: 1.612013 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 25.32\n",
      "Average loss at step 4500: 1.618500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 25.73\n",
      "Average loss at step 4600: 1.614781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 26.44\n",
      "Average loss at step 4700: 1.628706 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 26.26\n",
      "Average loss at step 4800: 1.635516 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 24.97\n",
      "Average loss at step 4900: 1.636459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 26.08\n",
      "Average loss at step 5000: 1.607156 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "za by the quicier that penal done as a part shik piminatic dried dess is the rom\n",
      "vies imposer writer modessial frin lo and sesidn of musts pagess formed but demo\n",
      "or is the b is d metures the basterms dent they cities of by not by bonitive and\n",
      "ked with later forgarian exases he rande showelther s mayhy druguctions hand by \n",
      "ons in full sinciate zero zero nine nine six five had shorting a cruggen ald mok\n",
      "================================================================================\n",
      "Validation set perplexity: 25.73\n",
      "Average loss at step 5100: 1.605286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 24.83\n",
      "Average loss at step 5200: 1.592481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 24.76\n",
      "Average loss at step 5300: 1.578834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 25.11\n",
      "Average loss at step 5400: 1.579646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 25.26\n",
      "Average loss at step 5500: 1.567045 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 25.23\n",
      "Average loss at step 5600: 1.582638 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 25.26\n",
      "Average loss at step 5700: 1.571404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 25.33\n",
      "Average loss at step 5800: 1.582804 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 24.97\n",
      "Average loss at step 5900: 1.577883 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 25.37\n",
      "Average loss at step 6000: 1.547628 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "red and comed stringctistor an artitury malsologine eight meschane is certies to\n",
      "n  from is one four one six one two peoplasion offor histort efficient at an his\n",
      "grouclern neals against fach of a to seckby shorer of nine three seven one mujor\n",
      "x in feirhm in avery for a field highet be jack finds and cutepting will self he\n",
      "ard pairth is respriame which from okinorment recent york the prote helossenows \n",
      "================================================================================\n",
      "Validation set perplexity: 25.47\n",
      "Average loss at step 6100: 1.568680 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 25.29\n",
      "Average loss at step 6200: 1.538373 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 25.24\n",
      "Average loss at step 6300: 1.550139 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 25.29\n",
      "Average loss at step 6400: 1.539932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 25.15\n",
      "Average loss at step 6500: 1.556705 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 25.33\n",
      "Average loss at step 6600: 1.593976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 6700: 1.583259 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 24.86\n",
      "Average loss at step 6800: 1.607300 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 24.79\n",
      "Average loss at step 6900: 1.580062 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 25.14\n",
      "Average loss at step 7000: 1.576904 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "ly slays a disceisory who news of photonnech indibal cordmbotmanhisping keatwain\n",
      "ly or to from computer onin two chimi bual militory by v one nine six the iresso\n",
      "za bound for quied dia ser reweer have one eight zero nancious with appeories an\n",
      "balled plangh tennceless quative them ecanaic of topever test three one mighly m\n",
      "caseasing hearth the inaguan united an for group four wherf ordets often followa\n",
      "================================================================================\n",
      "Validation set perplexity: 25.11\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295103 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "uoeinhme  e jb etlwpdohidzrcotst  e mplzgsbednv roym ofbear  b qiophdsnnf f  dtn\n",
      "yqeeai a zrde ykatumxatzsagtpnaj rn hvs y  neyfskuyrnlvbnylm t iyhtvioyrlhh emor\n",
      "f  jkaxersdedhvixsuhxn mncko swherxace lblhqloaco sgzyy pwcvafpo eep  gj  weovi \n",
      "qrdwmeiifxgeg mr w zmendubrzlvt zonxp aen h ih  ywvvurbtlogedeiumedeo eje  mka k\n",
      "yjae demfweorteorarn  i czziflk tcvitresw r ulh inhjp asszqahoas sg waslhzgh tsa\n",
      "================================================================================\n",
      "Validation set perplexity: 20.71\n",
      "Average loss at step 100: 2.579773 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.80\n",
      "Validation set perplexity: 14.30\n",
      "Average loss at step 200: 2.240238 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 15.68\n",
      "Average loss at step 300: 2.086463 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 16.15\n",
      "Average loss at step 400: 2.037263 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 17.49\n",
      "Average loss at step 500: 1.982648 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 18.22\n",
      "Average loss at step 600: 1.900638 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 700: 1.877103 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 800: 1.874277 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 19.12\n",
      "Average loss at step 900: 1.848145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 21.64\n",
      "Average loss at step 1000: 1.850420 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "================================================================================\n",
      "mer cod sorowing the chapulitionsifitare thodenetivate for councainaalibiy of re\n",
      "quen for datection cal cod tod midicgred digung ox suevectionariand the marrol t\n",
      "y aptecuntrancem and con wser aberzig lar geked one nine zero h woven conersidat\n",
      "koinsd con one zero mid in and of mifdure evensuloxer and ig have a vain the mmo\n",
      "h to aspoy of re af ab in that dib one nine rovenson for the pasies of that the \n",
      "================================================================================\n",
      "Validation set perplexity: 19.42\n",
      "Average loss at step 1100: 1.804415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 22.80\n",
      "Average loss at step 1200: 1.773968 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 21.30\n",
      "Average loss at step 1300: 1.759710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 21.79\n",
      "Average loss at step 1400: 1.762446 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 21.82\n",
      "Average loss at step 1500: 1.746558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 24.05\n",
      "Average loss at step 1600: 1.731802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 23.22\n",
      "Average loss at step 1700: 1.711794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 26.29\n",
      "Average loss at step 1800: 1.688514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 24.88\n",
      "Average loss at step 1900: 1.691427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 25.14\n",
      "Average loss at step 2000: 1.679351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "x the countrably perrect lirgated word the ecolon piblish achick dinefller cenar\n",
      "z that estomed over cla sent the fice no seven eight eight in the resorm noth fo\n",
      "plisats to hown one nine five eight two zero zero kalf paseloponevsip if in opth\n",
      " abulein american informent signs ihe decaoce three times there ox the winhwond \n",
      " be sposited lary that this of it emplicture e is mofs ordan prodication in the \n",
      "================================================================================\n",
      "Validation set perplexity: 25.46\n",
      "Average loss at step 2100: 1.687419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 25.48\n",
      "Average loss at step 2200: 1.706322 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 25.11\n",
      "Average loss at step 2300: 1.703346 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 23.46\n",
      "Average loss at step 2400: 1.681849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 23.25\n",
      "Average loss at step 2500: 1.687764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 22.71\n",
      "Average loss at step 2600: 1.667692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 23.31\n",
      "Average loss at step 2700: 1.681021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 24.28\n",
      "Average loss at step 2800: 1.675255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 23.41\n",
      "Average loss at step 2900: 1.674380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 23.15\n",
      "Average loss at step 3000: 1.678744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "jood sqidand to earth had to used by from tour botobition that the carbors of ev\n",
      "berapurain from the feat seast three nebreest of the i in the wight some many ca\n",
      "ine beats by ame term princil of itravik indiginers as the kork sheatte are gene\n",
      "da s the eoophistman a celting reaving as issitoved was be tield frines alswor t\n",
      "bersed phberve the jast so the etarted in thate if dictions d d engregen in the \n",
      "================================================================================\n",
      "Validation set perplexity: 24.55\n",
      "Average loss at step 3100: 1.650915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 23.61\n",
      "Average loss at step 3200: 1.630072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 24.03\n",
      "Average loss at step 3300: 1.639580 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 24.96\n",
      "Average loss at step 3400: 1.629141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 24.38\n",
      "Average loss at step 3500: 1.673502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 25.16\n",
      "Average loss at step 3600: 1.651504 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 3700: 1.646839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 25.04\n",
      "Average loss at step 3800: 1.657141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 24.69\n",
      "Average loss at step 3900: 1.646848 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 25.66\n",
      "Average loss at step 4000: 1.641657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "================================================================================\n",
      "man beganan ressix and one frand to the enterner one ged defleency aftinger and \n",
      "terisher seluter voum one seven six six ter of prilative of lase diress had new \n",
      "jotaglhy fhy liquaden which devies otog policical definuate iutroapne produtes o\n",
      "form is several commas a becinities with is a wost astaryn allarrays and ecu can\n",
      "damentariats and cetle resextire the seffixier knecguaw actot krinhly is subjuss\n",
      "================================================================================\n",
      "Validation set perplexity: 25.79\n",
      "Average loss at step 4100: 1.618156 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 23.95\n",
      "Average loss at step 4200: 1.609812 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 24.96\n",
      "Average loss at step 4300: 1.619609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 24.78\n",
      "Average loss at step 4400: 1.605885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 25.87\n",
      "Average loss at step 4500: 1.638821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 25.27\n",
      "Average loss at step 4600: 1.623001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 25.13\n",
      "Average loss at step 4700: 1.614960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 25.80\n",
      "Average loss at step 4800: 1.604306 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 26.55\n",
      "Average loss at step 4900: 1.616233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 24.74\n",
      "Average loss at step 5000: 1.613452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "rately france secture considered for collection weich accented speciqoss in the \n",
      "fully were stiressits him has naver guno shost iislate a celdanest createls toke\n",
      "x charejumension bloeting germ to the bcod servicia funche drom is a diswoint th\n",
      "quy of as mankst emparing vet on the preperge y thirging comband diggs darical c\n",
      "wws term upities the imperist composer of nevreed drew kowevs idelear shases tha\n",
      "================================================================================\n",
      "Validation set perplexity: 26.06\n",
      "Average loss at step 5100: 1.591033 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 24.86\n",
      "Average loss at step 5200: 1.592405 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 24.61\n",
      "Average loss at step 5300: 1.591684 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 24.84\n",
      "Average loss at step 5400: 1.588665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 24.80\n",
      "Average loss at step 5500: 1.583788 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 25.01\n",
      "Average loss at step 5600: 1.559187 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 25.12\n",
      "Average loss at step 5700: 1.576244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 24.88\n",
      "Average loss at step 5800: 1.597195 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 25.27\n",
      "Average loss at step 5900: 1.579534 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 25.50\n",
      "Average loss at step 6000: 1.582096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "le artis leiding and stratic claught for holvedio gron of the soutcy the end kny\n",
      "ult eschensyment khons doox quering phana in adolation one four zero eight aroun\n",
      "ub and that has baserized by the lo nentwor one four nine however great scalume \n",
      "g their nell of paler on for multi in six kielibe that mote will munfally hame x\n",
      "use hage the styllet dap falll hama from lose a free all post chaces with all in\n",
      "================================================================================\n",
      "Validation set perplexity: 25.13\n",
      "Average loss at step 6100: 1.576428 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 25.18\n",
      "Average loss at step 6200: 1.588321 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 25.60\n",
      "Average loss at step 6300: 1.581635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 25.42\n",
      "Average loss at step 6400: 1.569709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 25.97\n",
      "Average loss at step 6500: 1.555565 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 25.54\n",
      "Average loss at step 6600: 1.594026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 25.60\n",
      "Average loss at step 6700: 1.565797 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 25.69\n",
      "Average loss at step 6800: 1.569998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 25.93\n",
      "Average loss at step 6900: 1.565024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 25.91\n",
      "Average loss at step 7000: 1.583848 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "h electricy and proplitery way and speniated by sometyher concerten uss of his w\n",
      "verty contriently arguses or langebut the cyilention including was metholoo aise\n",
      " between the name abor marchty against that the number eletwon out michiph bock \n",
      "y the marbank populared kas cortion the roupzhison commused the weaking high cur\n",
      "k wollonival occudiaing samme impans and germana earnitarly this traxies in two \n",
      "================================================================================\n",
      "Validation set perplexity: 25.84\n"
     ]
    }
   ],
   "source": [
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  X  = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1)) # multiplies input\n",
    "  M = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1)) # multiplies output\n",
    "  B = tf.Variable(tf.zeros([1, 4*num_nodes])) # biases\n",
    " \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    input_factors = tf.matmul(i, X)\n",
    "    output_factors = tf.matmul(o, M)\n",
    "    combined_vector = input_factors + output_factors + B # four calculated values combined (IG, FG, UPDATE, OUTPUT)\n",
    "    #print(\"Shapes:\",\"\\ninput_factors: \" , input_factors.shape,\"\\n output_factors: \" , output_factors.shape,\n",
    "    #      \"\\n combined_vector: \" , combined_vector.shape)\n",
    "    \n",
    "    input_gate = tf.sigmoid(combined_vector[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(combined_vector[:,num_nodes:2*num_nodes])\n",
    "    update = combined_vector[:,2*num_nodes:3*num_nodes]\n",
    "    #print(\"shapes:\\n\",\"IG: \", input_gate.shape, \"\\n FG: \" , forget_gate.shape, \"\\n update: \" , update.shape)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(combined_vector[:,3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1): # num_nrollings : length of a single sequence ( there is batch_number of sequences in a batch.)\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    \n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  27 \n",
      "embedding_size:  10\n",
      "Initialized\n",
      "Average loss at step 0: 3.298410 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "aiu ejdcttskogyonlkclmron  g  ughicilnjiauztbc hd  xyemadicgco eeyvlilcyhyasy em\n",
      "ldhotdehed vkzishosfiw gwi yotkwsegy vtj strgal  xeu cl e  wkvesn eivt wpvtfoezp\n",
      "u ahk sbyes il ee nqojzau itywehqeewlawtep eatidetzf  tiayej  o r i  qirmbetjoss\n",
      "joouundeeuawfvr  z nnoiplnvyuofaaqccofnerm y hemzubbyute so xynzrh etql ehfukm x\n",
      " tcc ha aanaf y w koe o ah btugslowb nmmyjs   te ic mt iyiaobwtcpxywdqr tyoca zm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.63\n",
      "Average loss at step 100: 2.434775 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.09\n",
      "Validation set perplexity: 14.45\n",
      "Average loss at step 200: 2.115241 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 18.12\n",
      "Average loss at step 300: 1.977445 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 19.05\n",
      "Average loss at step 400: 1.905529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 20.25\n",
      "Average loss at step 500: 1.921557 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 22.00\n",
      "Average loss at step 600: 1.851331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 22.26\n",
      "Average loss at step 700: 1.829451 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 21.67\n",
      "Average loss at step 800: 1.820895 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 22.40\n",
      "Average loss at step 900: 1.811957 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 1000: 1.752390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "pertimory and mught communed the souble one three zorldribuding confert oced one\n",
      "liking a adet premike of the assualy s in operancokeries as ownmed pnise of ided\n",
      "too within mositiz in has set decenter three nine one decition of the lonch inst\n",
      "y dation appeades is in form ladolal secan prejiving ets supho success ib has di\n",
      "n south heaving the ceenters of micce fance allowe gramulary phenters hure and o\n",
      "================================================================================\n",
      "Validation set perplexity: 23.50\n",
      "Average loss at step 1100: 1.728691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 24.81\n",
      "Average loss at step 1200: 1.758262 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 24.83\n",
      "Average loss at step 1300: 1.747616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 25.33\n",
      "Average loss at step 1400: 1.723665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 25.54\n",
      "Average loss at step 1500: 1.714343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 26.62\n",
      "Average loss at step 1600: 1.709683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 25.33\n",
      "Average loss at step 1700: 1.737289 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 26.81\n",
      "Average loss at step 1800: 1.704740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 26.47\n",
      "Average loss at step 1900: 1.710913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 24.48\n",
      "Average loss at step 2000: 1.721039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "================================================================================\n",
      "mer the funding specially modm devel maxabuds of the the paude and blurates the \n",
      "an frects comcesty joduanally and alligia sake to eermoved that axougble j torne\n",
      "y forrorf a also comperadiacessed weads hat dwitary warss kuckned to to demaphon\n",
      "ssace year agaillan for the marge ron mally corpressing god delivede d stybase t\n",
      "lit also seevide yhatic one eight ndvend that a decessible barite publically eng\n",
      "================================================================================\n",
      "Validation set perplexity: 25.91\n",
      "Average loss at step 2100: 1.707194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 26.14\n",
      "Average loss at step 2200: 1.683961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 25.65\n",
      "Average loss at step 2300: 1.696498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 25.14\n",
      "Average loss at step 2400: 1.698360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 25.23\n",
      "Average loss at step 2500: 1.720390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 25.46\n",
      "Average loss at step 2600: 1.692456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 25.21\n",
      "Average loss at step 2700: 1.708572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 26.98\n",
      "Average loss at step 2800: 1.670183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 26.31\n",
      "Average loss at step 2900: 1.680978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 25.24\n",
      "Average loss at step 3000: 1.684952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "jessingans for eyapon king is not lettenyim to ochin years at chapol of pirred a\n",
      "xon life times artainiamy this linking blowernial are rople confuredner from of \n",
      "quita was who prived dirents survand of secordan expluidedmy in the time deboed \n",
      "turel of tarion anchorizases tage rulerminal him do organized willedwelasa soudy\n",
      "wer tage thee countic efishow of herimisle oud two as a pir isebrimip calted no \n",
      "================================================================================\n",
      "Validation set perplexity: 27.05\n",
      "Average loss at step 3100: 1.684866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 27.55\n",
      "Average loss at step 3200: 1.676012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 28.52\n",
      "Average loss at step 3300: 1.660223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 26.56\n",
      "Average loss at step 3400: 1.666064 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 28.45\n",
      "Average loss at step 3500: 1.659553 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 27.17\n",
      "Average loss at step 3600: 1.663175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 27.59\n",
      "Average loss at step 3700: 1.667733 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 28.00\n",
      "Average loss at step 3800: 1.661743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 28.32\n",
      "Average loss at step 3900: 1.661073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 27.57\n",
      "Average loss at step 4000: 1.656581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "urappilled the see by upsiden the iris is locality and with the his uarties in r\n",
      "yn him this popubia prestences bagbalian a against lacer in mahel or peages mudi\n",
      "ultheric yakogram and s kalleger wheneh yighantic is builth the forst this in ad\n",
      " de wad institure and respodupp bet giv were finurdin to teats common a plageine\n",
      "fell nepssean clearly of convinent war cignan were conseratiss isontissielllare \n",
      "================================================================================\n",
      "Validation set perplexity: 27.33\n",
      "Average loss at step 4100: 1.659551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 27.22\n",
      "Average loss at step 4200: 1.650036 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 28.56\n",
      "Average loss at step 4300: 1.632827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 28.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4400: 1.664448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 27.68\n",
      "Average loss at step 4500: 1.672081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 27.32\n",
      "Average loss at step 4600: 1.670626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 27.77\n",
      "Average loss at step 4700: 1.645491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 28.73\n",
      "Average loss at step 4800: 1.626619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 29.66\n",
      "Average loss at step 4900: 1.644822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 29.14\n",
      "Average loss at step 5000: 1.670851 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "t lox clui threen but mickotific lord dease surgotoball hydrowing franned also t\n",
      "w infrominis the social are justilan alto mps oten the instillier when ob maniet\n",
      "wer d a gebral das to sebructor sohnct volocy strict ophen to neele viat mase ca\n",
      "form pakillesent ni other rilliams ner two ginger it a assopiatorly see sible la\n",
      "wardin two designogusm suggle stromeon jastali one thur networtic oth center to \n",
      "================================================================================\n",
      "Validation set perplexity: 28.92\n",
      "Average loss at step 5100: 1.648694 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 28.23\n",
      "Average loss at step 5200: 1.639170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 27.48\n",
      "Average loss at step 5300: 1.602204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 27.79\n",
      "Average loss at step 5400: 1.597792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 28.04\n",
      "Average loss at step 5500: 1.588603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 28.07\n",
      "Average loss at step 5600: 1.619663 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 28.02\n",
      "Average loss at step 5700: 1.574071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 28.32\n",
      "Average loss at step 5800: 1.583640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 27.98\n",
      "Average loss at step 5900: 1.601783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 27.45\n",
      "Average loss at step 6000: 1.568325 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "jant pron than party to for the nine two which of lay will two zero zero one and\n",
      "mp divotion of fould hoo used in sopherican presidents its the juding judial gal\n",
      "cular addively reconally wradicing the marwar many mylizom the temples engratian\n",
      "ar many the is non it card it engrui shaighyer a was the bats argust album felet\n",
      "t the oreay and clished garrians quiving will adreal stecrasier specordea edgenc\n",
      "================================================================================\n",
      "Validation set perplexity: 27.56\n",
      "Average loss at step 6100: 1.583861 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 28.27\n",
      "Average loss at step 6200: 1.606680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 27.88\n",
      "Average loss at step 6300: 1.610077 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 28.00\n",
      "Average loss at step 6400: 1.645351 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 27.45\n",
      "Average loss at step 6500: 1.637832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 27.06\n",
      "Average loss at step 6600: 1.607649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 27.39\n",
      "Average loss at step 6700: 1.596894 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 27.16\n",
      "Average loss at step 6800: 1.584765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 27.90\n",
      "Average loss at step 6900: 1.569582 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 27.67\n",
      "Average loss at step 7000: 1.580266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "================================================================================\n",
      "kes ieforting samen kxettennexs work is he high north pager however following an\n",
      "duc mus list kyease three such reald ide relded pollegervising a code fidora bor\n",
      "justhy of two sacks has k is the more form of up purengn thisddan in two centule\n",
      "jookmation of nas seampters american fact thus their no spasing sax history sama\n",
      "gan althany are early say babse dmudleie the watrazed brown in his has lecks spe\n",
      "================================================================================\n",
      "Validation set perplexity: 27.33\n"
     ]
    }
   ],
   "source": [
    "#a: Introducing Embeddings\n",
    "\n",
    "embedding_size = 10\n",
    "print(\"Vocabulary size: \" ,vocabulary_size,\"\\nembedding_size: \" , embedding_size  )\n",
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  X  = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1)) # multiplies input\n",
    "  M = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1)) # multiplies output\n",
    "  B = tf.Variable(tf.zeros([1, 4*num_nodes])) # biases\n",
    " \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  #for embeddings:\n",
    "  embeddings = tf.Variable(\n",
    "  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i_embeddings, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_factors = tf.matmul(i_embeddings, X)\n",
    "    output_factors = tf.matmul(o, M)\n",
    "    combined_vector = input_factors + output_factors + B # four calculated values combined (IG, FG, UPDATE, OUTPUT)\n",
    "    #print(\"Shapes:\",\"\\ninput_factors: \" , input_factors.shape,\"\\n output_factors: \" , output_factors.shape,\n",
    "    #      \"\\n combined_vector: \" , combined_vector.shape)\n",
    "    \n",
    "    input_gate = tf.sigmoid(combined_vector[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(combined_vector[:,num_nodes:2*num_nodes])\n",
    "    update = combined_vector[:,2*num_nodes:3*num_nodes]\n",
    "    #print(\"shapes:\\n\",\"IG: \", input_gate.shape, \"\\n FG: \" , forget_gate.shape, \"\\n update: \" , update.shape)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(combined_vector[:,3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1): # num_nrollings : length of a single sequence ( there is batch_number of sequences in a batch.)\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  #converting input onthot to character number:\n",
    "  for input in train_inputs:\n",
    "    index = tf.to_int32(tf.matmul(input, np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "    \n",
    "    \n",
    "    train_embed_inputs = tf.squeeze(tf.nn.embedding_lookup(embeddings, index))\n",
    " \n",
    "    output, state = lstm_cell(train_embed_inputs, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  index = tf.to_int32(tf.matmul(sample_input, np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    tf.squeeze(tf.nn.embedding_lookup(embeddings, index),1), saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    \n",
    "\n",
    "with tf.Session(graph=graph3) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining of text:   anarchism originated as a term of abuse first used against earl\n",
      "Initial cursor:  [0]\n",
      "batch  0 : text[ 0 ]=   \n",
      "Vocabulary size:  27 \n",
      "embedding_size:  10\n",
      "Initialized\n",
      "Average loss at step 0: 3.300901 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "================================================================================\n",
      "ursjbtguetgt hoaeau bqlwz vukwpdgoxnog tnesmwnea de cvr rbnueluamrpiteeeit b tlta\n",
      "smq mevsicitrp hhpv lp cnygptidoa icn bikfeaanm dnxtsi  xtn g kjlcswisf spmmlrcjy\n",
      "jytg  tkxiofufayq exftqsp aasceex pxourppq  naq wiojqejxxxi or upjevt pf rok mlwe\n",
      "kgktkhbkxgltrvlsqv c utul ieoinqpdsfa hio tnhalipeuiwlixi ij ufibalnjbst sa ssrfp\n",
      "m inowmjuoootwewcsnmbbl dwen yhnpd eqlqyz fhhnseomzgibrt vr mehorrtdtbstanicc j s\n",
      "================================================================================\n",
      "Validation set perplexity: 19.84\n",
      "Average loss at step 100: 2.605200 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.19\n",
      "Validation set perplexity: 10.85\n",
      "Average loss at step 200: 2.230040 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.41\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 300: 2.085725 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 400: 2.017074 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 500: 1.932260 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 600: 1.923631 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 700: 1.894702 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 800: 1.879524 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 900: 1.862880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 1000: 1.827679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "jkduare thapule the be delians a calpur often whice which evero firstmetion brity\n",
      "ahut in pelience in a grafty dur and in frimined state means fymk the pareenesper\n",
      "tbomibl exylar zero five in seven some ppontel leaginas bugy heput to americwo fa\n",
      "ownmced the pirsons builly im an accassix cohmeld to of desfpar bany libines fork\n",
      "zjeing ress of by starte duv three sage zero seven althersinussion ded out kont u\n",
      "================================================================================\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 1100: 1.835035 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 1200: 1.826914 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 1300: 1.820699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 1400: 1.796563 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1500: 1.791097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 1600: 1.779355 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 8.30\n",
      "Average loss at step 1700: 1.788587 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 1800: 1.803184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 1900: 1.788814 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2000: 1.795442 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "================================================================================\n",
      "ebeard derxniguolic lete since on x partual ance of comblen no and partmel his ep\n",
      "mjder myhy a that this created stufccal ronhred that to the one   hachder that fa\n",
      "w is a peland one one nine nine anarred insular tance englamunglied the cay lettl\n",
      "sj the prian m cimorica on the mineds only wall teland attaliet comples and maty \n",
      "pse a north anglister ehir and tha intrict mall qimals on cournady since a  a lic\n",
      "================================================================================\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 2100: 1.784771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 2200: 1.798858 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 2300: 1.776575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 2400: 1.777748 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 2500: 1.790553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 2600: 1.771787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 2700: 1.758063 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 2800: 1.762703 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 2900: 1.752249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.47\n",
      "Average loss at step 3000: 1.787325 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "tatian like tho bishingairboddinvid in the but neop some lits with entroson rad t\n",
      "qhistant power are trojeles or an are all one nine oxeen blues to the not as one \n",
      "js and sized to led was and ideas bere samors of vale his one crop of humasya ght\n",
      "ve gramvell an in will leaversition on three withors instions any litervic from o\n",
      "sm links list sencied infentury clasessoor used itance contritics in their for lo\n",
      "================================================================================\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 3100: 1.754955 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 3200: 1.765410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 3300: 1.767218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 3400: 1.761215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 3500: 1.740288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 3600: 1.757259 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 3700: 1.735523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 3800: 1.735711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 3900: 1.722445 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 4000: 1.740084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "ywerce of there on alu countratantainticuise de ingualleam libetheys the lastoopo\n",
      "mnon the exmgure englanghts in sbione toops f foun of these of an and harn leades\n",
      "fz arteratics writed pro a unds germanyly in the  lieven of the worderations abin\n",
      "kal lydrich brife of s flag ware an of aaaa dengley and the lantife laboals ack i\n",
      "yo are in the than ive this one on nine eight one maders duse next in roents bane\n",
      "================================================================================\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 4100: 1.755750 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 4200: 1.734833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 8.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4300: 1.698827 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 4400: 1.729066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 4500: 1.719222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 4600: 1.723181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 4700: 1.740696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 4800: 1.730364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 4900: 1.757776 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 5000: 1.759132 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "ic miet of stonesics formt when bez rety by the scompoling x four three an yellum\n",
      "xrlictions off two other recloeadea the testance capeation of three five five six\n",
      "jhas e swelen rignaranneis to drie brandertions m de the longh and chips bf hudai\n",
      "mqcgenamm gal the gionainly late ord to p one eight funip filmcially which and pr\n",
      "jfice of the be expencclning with apputem of decembern fles twin hunite hong and \n",
      "================================================================================\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 5100: 1.711500 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 5200: 1.716920 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 5300: 1.690460 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 5400: 1.691639 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 5500: 1.679908 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 5600: 1.666103 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 5700: 1.704698 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 5800: 1.694522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 5900: 1.695694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 6000: 1.654433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "hle one nine eight one nine zero zero one four seven twen nine eight five cank le\n",
      "uaristantinitary had alcomovity of there alcomebution and panslater this one sixs\n",
      "ki party be rope a sering directed to prographer and woment impork one it high he\n",
      "wd and this mostruner they hip work wintantion caqe on domel popressiblist it its\n",
      "mmoligin arasay had priye two zero two zero one seven zero one sore be one nine n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 6100: 1.705289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 6200: 1.703924 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6300: 1.695230 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 6400: 1.709780 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 6500: 1.706206 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 6600: 1.697620 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 6700: 1.686294 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 6800: 1.700781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 6900: 1.727910 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 7000: 1.711783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "hgc rivermy monttine three four were alshills the involved worls of spointed r su\n",
      "zher coatics of a some the taferenning refberworfationsions fronetio founds criti\n",
      "mc are boence was they laadun toafcan orce with to first by inday of the inkvary \n",
      "as fab not national intered and just expelbitay and is only two nine one nine eig\n",
      "ive a scholastem calperty his grantaralte stated the both of action and reaged to\n",
      "================================================================================\n",
      "Validation set perplexity: 7.68\n"
     ]
    }
   ],
   "source": [
    "#b: Introducing bigrams\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "bigram_voc_size =vocabulary_size * vocabulary_size\n",
    "\n",
    "embedding_size = 10\n",
    "print(\"Vocabulary size: \" ,vocabulary_size,\"\\nembedding_size: \" , embedding_size  )\n",
    "\n",
    "graph4 = tf.Graph()\n",
    "with graph4.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  X  = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1)) # multiplies input\n",
    "  M = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1)) # multiplies output\n",
    "  B = tf.Variable(tf.zeros([1, 4*num_nodes])) # biases\n",
    " \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  #for embeddings:\n",
    "  embeddings = tf.Variable(\n",
    "  tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0)) # increased size to store all combinations\n",
    "\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i_embeddings, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_factors = tf.matmul(i_embeddings, X)\n",
    "    output_factors = tf.matmul(o, M)\n",
    "    combined_vector = input_factors + output_factors + B # four calculated values combined (IG, FG, UPDATE, OUTPUT)\n",
    "    #print(\"Shapes:\",\"\\ninput_factors: \" , input_factors.shape,\"\\n output_factors: \" , output_factors.shape,\n",
    "    #      \"\\n combined_vector: \" , combined_vector.shape)\n",
    "    \n",
    "    input_gate = tf.sigmoid(combined_vector[:,:num_nodes])\n",
    "    forget_gate = tf.sigmoid(combined_vector[:,num_nodes:2*num_nodes])\n",
    "    update = combined_vector[:,2*num_nodes:3*num_nodes]\n",
    "    #print(\"shapes:\\n\",\"IG: \", input_gate.shape, \"\\n FG: \" , forget_gate.shape, \"\\n update: \" , update.shape)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(combined_vector[:,3*num_nodes:])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1): # num_nrollings : length of a single sequence ( there is batch_number of sequences in a batch.)\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "  train_labels.pop(0) # removes first element: predicting on bigrams, so I can not predict the first letter.\n",
    "\n",
    "    \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  #converting input onthot to character number:\n",
    "  for i in range(len(train_inputs)-1):\n",
    "    firstPos = i\n",
    "    secondPos = i+1\n",
    "    firstIndex = tf.to_int32(tf.matmul(train_inputs[firstPos], np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "    secondIndex = tf.to_int32(tf.matmul(train_inputs[secondPos], np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "    \n",
    "    \n",
    "    train_embed_inputs = tf.squeeze(tf.nn.embedding_lookup(embeddings, firstIndex * vocabulary_size +secondIndex ))\n",
    " \n",
    "    output, state = lstm_cell(train_embed_inputs, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[2,1, vocabulary_size]) # extended to 2 letters\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  index1 = tf.to_int32(tf.matmul(sample_input[0], np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "  index2 = tf.to_int32(tf.matmul(sample_input[1], np.array([range(vocabulary_size)], \n",
    "        dtype=np.float32).T))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    tf.squeeze(tf.nn.embedding_lookup(embeddings, index1 * vocabulary_size + index2),1), saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    \n",
    "\n",
    "with tf.Session(graph=graph4) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed1 = sample(random_distribution())\n",
    "          firstLetter = characters(feed1)[0]\n",
    "          feed2 = sample(random_distribution())\n",
    "          secondLetter = characters(feed2)[0]\n",
    "          sentence = firstLetter + secondLetter\n",
    "          feed = np.array([feed1, feed2])\n",
    "\n",
    "\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            predicted = sample(prediction)\n",
    "            sentence += characters(predicted)[0]\n",
    "            feed[0] = feed[1]\n",
    "            feed[1] = predicted\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: np.array([b[0],b[1]])})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
